{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\" Reads dataset csv and returns pandas dataframe \"\"\"\n",
    "    \n",
    "    filepath = \"Resources/charity_data.csv\"\n",
    "\n",
    "    df = pd.read_csv(filepath, encoding=\"utf-8\", low_memory=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(a_df):\n",
    "    \"\"\" Returns deduped, na-dropped, index-reset dataframe \"\"\"    \n",
    "    \n",
    "    a_df = a_df.drop_duplicates()   \n",
    "        \n",
    "    a_df = a_df.dropna()\n",
    "        \n",
    "    a_df = a_df.reset_index(drop=True)\n",
    "    \n",
    "    return a_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_dataset(a_df):\n",
    "    \"\"\" Provides summary info and visualizations of dataset \"\"\"\n",
    "    \n",
    "    print(a_df.info())\n",
    "           \n",
    "    # Determine the number of unique values in each column.\n",
    "\n",
    "    for col in a_df.columns:\n",
    "        print(f\"{col} \\n{a_df[col].value_counts()}\\n\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataset(load_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examine_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial columns\n",
    "\n",
    "df = df.drop(columns=[\"EIN\", \"NAME\", \"STATUS\", \"SPECIAL_CONSIDERATIONS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cats(a_col, a_cutoff):\n",
    "    \"\"\" Inputs are a series and a cutoff value for 'Other' \"\"\"\n",
    "    \n",
    "    print(f\"BEFORE: \\n\\n{df[a_col].value_counts()}\\n\\n\")\n",
    "\n",
    "    types_to_replace = (df[a_col].value_counts().loc[lambda x: x < int(a_cutoff)]).keys().tolist()\n",
    "\n",
    "    for code in types_to_replace:        \n",
    "        df[a_col] = df[a_col].replace(code, \"Other\")\n",
    "\n",
    "    # Check to make sure binning was successful\n",
    "\n",
    "    print(f\"AFTER: \\n\\n{df[a_col].value_counts()}\\n\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_cats(\"AFFILIATION\", 16_000)\n",
    "\n",
    "reduce_cats(\"APPLICATION_TYPE\", 27_000)\n",
    "\n",
    "reduce_cats(\"ASK_AMT\", 25_000)\n",
    "\n",
    "reduce_cats(\"CLASSIFICATION\", 17_000)\n",
    "\n",
    "reduce_cats(\"INCOME_AMT\", 24_000)\n",
    "\n",
    "reduce_cats(\"ORGANIZATION\", 11_000)\n",
    "\n",
    "reduce_cats(\"USE_CASE\", 5700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_df(a_df):\n",
    "    \"\"\" Returns one-hot encoded dataframe \"\"\"\n",
    "    \n",
    "    categorical_list = a_df.dtypes[a_df.dtypes == \"object\"].index.tolist() \n",
    "    \n",
    "    print(f\"CATEGORIES FOR EACH CATEGORICAL FEATURE ENCODED:\\n\\n{a_df[categorical_list].nunique()}\\n\\n\")\n",
    "    \n",
    "    concat_list = []\n",
    "    \n",
    "    for categorical in categorical_list:\n",
    "        \n",
    "        concat_list.append(pd.get_dummies(a_df[categorical], prefix=categorical, prefix_sep='_'))        \n",
    "    \n",
    "    concat_list.append(a_df[\"IS_SUCCESSFUL\"])\n",
    "\n",
    "    return pd.concat(concat_list, axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df = encode_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(a_df):\n",
    "    \"\"\" Make X,y ... train_test_split ... scale, fit and transform \"\"\"\n",
    "    \n",
    "    # Split our preprocessed data into our features and target arrays\n",
    "\n",
    "    y = enc_df[\"IS_SUCCESSFUL\"].values\n",
    "    X = enc_df.drop([\"IS_SUCCESSFUL\"], axis=1).values\n",
    "\n",
    "    # Split the preprocessed data into a training and testing dataset\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42) \n",
    "    \n",
    "    # Create a StandardScaler instances\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the StandardScaler\n",
    "\n",
    "    X_scaler = scaler.fit(X_train)\n",
    "\n",
    "    # Scale the data\n",
    "\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    print(f\"TRAIN SCALED SHAPE: {X_train_scaled.shape}\")\n",
    "    print(f\"TEST SCALED SHAPE: {X_test_scaled.shape}\")\n",
    "\n",
    "    input_dim = X_train_scaled.shape[1]  \n",
    "    \n",
    "    return input_dim, X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, X_train_scaled, X_test_scaled, y_train, y_test = pre_process(enc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn(input_dim=14, num_layers=2, num_units=5, num_epochs=5):\n",
    "    \"\"\" Makes sequential nn, compiles, fits, saves, and reports on loss and accuracy \"\"\"    \n",
    "    \n",
    "    nn = tf.keras.models.Sequential()\n",
    "\n",
    "    # First layer \n",
    "    \n",
    "    nn.add(tf.keras.layers.Dense(units=num_units, input_dim=input_dim, activation=\"relu\"))\n",
    "    \n",
    "    # Hidden layers\n",
    "    \n",
    "    for layer in range(1, num_layers):\n",
    "        \n",
    "        nn.add(tf.keras.layers.Dense(units=num_units, activation=\"relu\"))\n",
    "\n",
    "    # Output layer\n",
    "\n",
    "    nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    \n",
    "    nn.summary()\n",
    "    \n",
    "    nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    fit_model = nn.fit(X_train_scaled, y_train, epochs=num_epochs)   \n",
    "    \n",
    "    model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "    \n",
    "    print(f\"\\n\\nLoss: {model_loss}, Accuracy: {model_accuracy}\")    \n",
    "\n",
    "    nn.save(\"AlphabetSoupCharity.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_nn(14, 5, 60, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "\n",
    "def create_model(hp):\n",
    "    \n",
    "    # Instantiate a Sequential model\n",
    "    \n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers.\n",
    "    \n",
    "    activation = hp.Choice('activation', ['relu','tanh','sigmoid'])\n",
    "    \n",
    "    # Allow kerastuner to decide the number of neurons in first layer and also\n",
    "    # the activation function. \n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=10,\n",
    "        step=2), activation=activation, input_dim=35))\n",
    "\n",
    "    # Allow kerastuner to decide the number of hidden layers and number of \n",
    "    # neurons in each one\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 1, 4)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=10,\n",
    "            step=3),\n",
    "            activation=activation))\n",
    "    \n",
    "    # Define the output layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best():\n",
    "    \"\"\" Uses keras-tuner to find best model specs \"\"\"\n",
    "    \n",
    "    tuner = kt.Hyperband(\n",
    "        create_model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_epochs=10,\n",
    "        hyperband_iterations=2)\n",
    "    \n",
    "    # Run the kerastuner search for best hyperparameters\n",
    "\n",
    "    tuner.search(X_train_scaled, y_train, epochs=20, validation_data=(X_test_scaled, y_test))\n",
    "    \n",
    "     # Get best model hyperparameters\n",
    " \n",
    "    best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "    print(best_hyper.values)\n",
    "    \n",
    "     # Evaluate best model against full test data\n",
    " \n",
    "    best_model = tuner.get_best_models(1)[0]\n",
    "    model_loss, model_accuracy = best_model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "    print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "    \n",
    "     # Summarize the best model\n",
    " \n",
    "    print(best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=9, input_dim=35, activation=\"tanh\"))\n",
    "\n",
    "# Second hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=7, activation=\"tanh\"))\n",
    "\n",
    "# Third hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"tanh\"))\n",
    "\n",
    "# Fourth hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=9, activation=\"tanh\"))\n",
    "\n",
    "# Fifth hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=7, activation=\"tanh\"))\n",
    "\n",
    "\n",
    "# Output layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "\n",
    "nn.summary()\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "\n",
    "# Train the model\n",
    "\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=50) \n",
    "\n",
    "# Evaluate the model using the test data\n",
    "\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "# Export our model to HDF5 file\n",
    "\n",
    "nn.save(\"nn_optimized.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
