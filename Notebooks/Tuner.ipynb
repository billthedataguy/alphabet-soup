{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96db5207",
   "metadata": {},
   "source": [
    "# Tuning with keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e43a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras_tuner as kt\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f67587",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575c3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\" Reads dataset csv and returns pandas dataframe \"\"\"\n",
    "    \n",
    "    filepath = \"../Resources/charity_data.csv\"\n",
    "\n",
    "    df = pd.read_csv(filepath, encoding=\"utf-8\", low_memory=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54455e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(a_df):\n",
    "    \"\"\" Returns deduped, na-dropped, index-reset dataframe \"\"\"    \n",
    "    \n",
    "    a_df = a_df.drop_duplicates()   \n",
    "        \n",
    "    a_df = a_df.dropna()\n",
    "    \n",
    "    a_df = a_df.drop(columns=[\"EIN\", \"NAME\"])\n",
    "        \n",
    "    a_df = a_df.reset_index(drop=True)\n",
    "    \n",
    "    return a_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134ba416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_dataset(a_df):\n",
    "    \"\"\" Provides summary info and visualizations of dataset \"\"\"\n",
    "    \n",
    "    a_df.info()\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "                  \n",
    "    # Determine the number of unique values in each column.\n",
    "\n",
    "    for col in a_df.columns:\n",
    "        if (a_df[col].nunique() > 10):\n",
    "            print(f\"{col}\\n\\n{a_df[col].value_counts()}\\n\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cats(a_col, a_cutoff):\n",
    "    \"\"\" Inputs are a series and a cutoff value for 'Other' \"\"\"\n",
    "    \n",
    "    print(f\"BEFORE: \\n\\n{df[a_col].value_counts()}\\n\\n\")\n",
    "\n",
    "    types_to_replace = (df[a_col].value_counts().loc[lambda x: x < int(a_cutoff)]).keys().tolist()\n",
    "\n",
    "    for code in types_to_replace:        \n",
    "        df[a_col] = df[a_col].replace(code, \"Other\")\n",
    "\n",
    "    # Check to make sure binning was successful\n",
    "\n",
    "    print(f\"AFTER: \\n\\n{df[a_col].value_counts()}\\n\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_df(a_df):\n",
    "    \"\"\" Returns one-hot encoded dataframe \"\"\"\n",
    "    \n",
    "    categorical_list = a_df.dtypes[a_df.dtypes == \"object\"].index.tolist() \n",
    "    \n",
    "    print(f\"CATEGORIES FOR EACH CATEGORICAL FEATURE ENCODED:\\n\\n{a_df[categorical_list].nunique()}\\n\\n\")\n",
    "    \n",
    "    concat_list = []\n",
    "    \n",
    "    for categorical in categorical_list:\n",
    "        \n",
    "        concat_list.append(pd.get_dummies(a_df[categorical], prefix=categorical, prefix_sep='_'))        \n",
    "    \n",
    "    concat_list.append(a_df[\"IS_SUCCESSFUL\"])\n",
    "\n",
    "    return pd.concat(concat_list, axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afa5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(a_df):\n",
    "    \"\"\" Make X,y ... train_test_split ... scale, fit and transform \"\"\"\n",
    "    \n",
    "    # Split our preprocessed data into our features and target arrays\n",
    "\n",
    "    y = enc_df[\"IS_SUCCESSFUL\"].values\n",
    "    X = enc_df.drop([\"IS_SUCCESSFUL\"], axis=1).values\n",
    "\n",
    "    # Split the preprocessed data into a training and testing dataset\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "    \n",
    "    # Create a StandardScaler instances\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the StandardScaler\n",
    "\n",
    "    X_scaler = scaler.fit(X_train)\n",
    "\n",
    "    # Scale the data\n",
    "\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    print(f\"TRAIN SCALED SHAPE: {X_train_scaled.shape}\")\n",
    "    print(f\"TEST SCALED SHAPE: {X_test_scaled.shape}\")\n",
    "\n",
    "    input_dim = X_train_scaled.shape[1]  \n",
    "    \n",
    "    return input_dim, X_train_scaled, X_test_scaled, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6351f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    \n",
    "    # NB: change input_dim as needed!\n",
    "    \n",
    "    # Instantiate a Sequential model\n",
    "    \n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers.\n",
    "    \n",
    "    activation = hp.Choice('activation', ['relu','tanh','sigmoid'])\n",
    "    \n",
    "    # Allow kerastuner to decide the number of neurons in first layer and also\n",
    "    # the activation function. \n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=1,\n",
    "        max_value=10,\n",
    "        step=1), activation=activation, input_dim=43))\n",
    "\n",
    "    # Allow kerastuner to decide the number of hidden layers and number of \n",
    "    # neurons in each one\n",
    "    \n",
    "    for i in range(hp.Int('num_layers', 1, 4)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=1,\n",
    "            max_value=10,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "    \n",
    "    # Define the output layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d211f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best():\n",
    "    \"\"\" Uses keras-tuner to find best model specs \"\"\"\n",
    "    \n",
    "    tuner = kt.Hyperband(\n",
    "        create_model,\n",
    "        objective=\"val_accuracy\",\n",
    "        max_epochs=10,\n",
    "        hyperband_iterations=2)\n",
    "    \n",
    "    # Run the kerastuner search for best hyperparameters\n",
    "\n",
    "    tuner.search(X_train_scaled, y_train, epochs=20, validation_data=(X_test_scaled, y_test))\n",
    "    \n",
    "     # Get best model hyperparameters\n",
    " \n",
    "    best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "    print(best_hyper.values)\n",
    "    \n",
    "     # Evaluate best model against full test data\n",
    " \n",
    "    best_model = tuner.get_best_models(1)[0]\n",
    "    model_loss, model_accuracy = best_model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "    print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "    \n",
    "     # Summarize the best model\n",
    " \n",
    "    print(best_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e314274",
   "metadata": {},
   "source": [
    "# Call functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataset(load_dataset())\n",
    "\n",
    "examine_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495576f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin columns with > 10 unique values\n",
    "\n",
    "reduce_cats(\"APPLICATION_TYPE\", 500)\n",
    "\n",
    "reduce_cats(\"ASK_AMT\", 25_000)\n",
    "\n",
    "reduce_cats(\"CLASSIFICATION\", 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af702cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec0ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df = encode_df(df)\n",
    "\n",
    "enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039556b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim, X_train_scaled, X_test_scaled, y_train, y_test = pre_process(enc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec754a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0375fef",
   "metadata": {},
   "source": [
    "# Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f8f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=7, input_dim=27, activation=\"tanh\"))\n",
    "\n",
    "# First hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=9, activation=\"tanh\"))\n",
    "\n",
    "# Second hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=9, activation=\"tanh\"))\n",
    "\n",
    "# Third hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=7, activation=\"tanh\"))\n",
    "\n",
    "# Output layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "\n",
    "nn.summary()\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "\n",
    "# Train the model\n",
    "\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=10) \n",
    "\n",
    "# Evaluate the model using the test data\n",
    "\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "# Export our model to HDF5 file\n",
    "\n",
    "nn.save(\"../Models/nn_optimized.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d06727",
   "metadata": {},
   "source": [
    "# Best result: \n",
    "\n",
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
