{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383d8362",
   "metadata": {},
   "source": [
    "# Report on the Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bab696",
   "metadata": {},
   "source": [
    "## Overview of the analysis \n",
    "\n",
    "This analysis explains my attempts to construct a deep learning neural network to predict which applicants will be successful with funding, using the Alphabet Soup dataset. While deep learning neural networks seemed promising for this data problem, tree classifier models slightly outperformed them and are worth further exploration.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d874fe",
   "metadata": {},
   "source": [
    "## Results: \n",
    "\n",
    "**Data Preprocessing**\n",
    "\n",
    "- Target variable: IS_SUCCESSFUL \n",
    "\n",
    "- Feature variables: APPLICATION_TYPE, AFFILIATION, CLASSIFICATION, USE_CASE, ORGANIZATION, STATUS, INCOME_AMT, SPECIAL_CONSIDERATIONS, ASK_AMT\n",
    "\n",
    "- Useless variables: EIN, NAME\n",
    "\n",
    "**Compiling, Training, and Evaluating the Model**\n",
    "\n",
    "On my first attempt, my preprocessed data had 43 inputs. I created a neural network with input_dim of 43, 2 hidden layers with 100 units each, and 10 epochs. I chose 100 units as a value because it is advisable to start with a number 2 to 3 times as big as the input dimension. 2 layers and 10 epochs were a modest and sensible starting point for benchmarking. Relu was the activation function for all of the layers except the last one, which used sigmoid. \n",
    "\n",
    "My best performance with these specifications was: \n",
    "\n",
    "- **Loss: 0.5517758131027222, Accuracy: 0.729912519454956**\n",
    "\n",
    "While decent, this result did not meet our desired threshold of 75% accuracy.  To address this, I fed the keras-tuner all of the same specifications from above, hoping for a promising lead on the best model. Its output for the best model could not be replicated with the specifications it suggested, but they led me to experiment with similar settings. I ended up running 100 epochs with an input layer of 9 units in relu activation, and 2 hidden layers of 9 units, one in relu and one in tanh activation. The output layer used the sigmoid function.  With these parameters, I was able to achieve this performance:\n",
    "\n",
    "- **Loss: 0.5544543266296387, Accuracy: 0.7315452098846436**\n",
    "\n",
    "**Reflections**\n",
    "\n",
    "At this point I was at a loss as to how to improve this model's performance.  I went back to the drawing board ... which of these \"features\" were really contributing to the IS_SUCCESSFUL values? I pursued a correlation matrix and the feature_importances_ from Random Forest and Extra Trees classifiers.   \n",
    "\n",
    "- The correlation matrix heatmap revealed a little intelligence. AFFILIATION_Independent and AFFILIATION_CompanySponsored were the strongest positive and negative correlations with IS_SUCCESSFUL, respectively.\n",
    "\n",
    "- Random Forest and Extra Trees Classifiers offered more insight, especially when plotting the feature_importances_ suggested by each. Not only did each classifier get a higher score than my neural networks on both training and testing, but these scores increased slightly when using SelectFromModel and rerunning the classifiers.\n",
    "\n",
    "\n",
    "- **RandomForestClassifier Training Score: 0.7471233089721661**\n",
    "- **RandomForestClassifier Testing Score: 0.7258309037900874**\n",
    "\n",
    "\n",
    "- **SelectFromModel RandomForestClassifier Training Score: 0.7406313170580003**\n",
    "- **SelectFromModel RandomForestClassifier Testing Score: 0.7334784636914943**\n",
    "\n",
    "\n",
    "- **ExtraTreesClassifier Training Score: 0.7471233089721661**\n",
    "- **ExtraTreesClassifier Testing Score: 0.7258309037900874**\n",
    "\n",
    "\n",
    "- **SelectFromModel ExtraTreesClassifier Training Score: 0.7406313170580003**\n",
    "- **SelectFromModel ExtraTreesClassifier Testing Score: 0.732856476442233**\n",
    "\n",
    "\n",
    "**Summary**\n",
    "\n",
    "All told, my neural network underperformed the SelectFromModel Random Forest and Extra Trees Classifiers, but only slightly. Keras-tuner proved to be less illuminating than I had at first hoped, but did give me the opportunity to adjust several parameters in the deep neural network models. The somewhat better performance of the tree classifiers lead me to recommend these models - and their close cousins - as worthy of further exploration to improve model scores.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
