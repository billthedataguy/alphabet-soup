{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383d8362",
   "metadata": {},
   "source": [
    "# Report on the Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bab696",
   "metadata": {},
   "source": [
    "## Overview of the analysis \n",
    "\n",
    "This analysis explains my attempts to construct a deep learning neural network to predict which applicants will be successful with funding, using the Alphabet Soup dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d874fe",
   "metadata": {},
   "source": [
    "## Results: \n",
    "\n",
    "**Data Preprocessing**\n",
    "\n",
    "- Target variable: IS_SUCCESSFUL \n",
    "\n",
    "- Feature variables: APPLICATION_TYPE, AFFILIATION, CLASSIFICATION, USE_CASE, ORGANIZATION, STATUS, INCOME_AMT, SPECIAL_CONSIDERATIONS, ASK_AMT\n",
    "\n",
    "- Useless variables: EIN, NAME\n",
    "\n",
    "**Compiling, Training, and Evaluating the Model**\n",
    "\n",
    "On my first attempt, my preprocessed data had 43 inputs. I created a neural network with input_dim of 43, 2 hidden layers with 100 units each, and 10 epochs. I chose 100 units as a value because it is advisable to start with a number 2 to 3 times as big as the input dimension. 2 layers and 10 epochs were a modest and sensible starting point for benchmarking. Relu was the activation function for all of the layers except the last one, which used sigmoid. \n",
    "\n",
    "My best performance with these specifications was: \n",
    "\n",
    "- **Loss: 0.5545602440834045, Accuracy: 0.7287463545799255**\n",
    "\n",
    "While decent, this result did not meet our desired threshold of 75% accuracy.  To address this, I fed the keras-tuner in Google Colaboratory all of the same specifications from above, hoping for a promising lead on the best model. Its output for the best model could not be replicated with the specifications it suggested, but they led me to experiment with similar settings. I ended up running 100 epochs with an input layer of 9 units in relu activation, and 2 hidden layers of 9 units, one in relu and one in tanh activation. The output layer used the sigmoid function.  With these parameters, I was able to achieve this performance:\n",
    "\n",
    "- **Loss: 0.5544543266296387, Accuracy: 0.7315452098846436**\n",
    "\n",
    "**Reflections**\n",
    "\n",
    "At this point I was at a loss as to how to improve this model's performance.  I went back to the drawing board ... which of these \"features\" were really contributing to the IS_SUCCESSFUL values?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d678e75",
   "metadata": {},
   "source": [
    "How many neurons, layers, and activation functions did you select for your neural network model, and why?\n",
    "Were you able to achieve the target model performance?\n",
    "What steps did you take in your attempts to increase model performance?\n",
    "\n",
    "**Summary**: Summarize the overall results of the deep learning model. Include a recommendation for how a different model could solve this classification problem, and then explain your recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01270e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_selection import chi2, f_classif, SelectFromModel, SelectKBest \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import and read the charity_data.csv\n",
    "\n",
    "df = pd.read_csv(\"../Resources/charity_data.csv\")\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "\n",
    "df = df.drop(columns=[\"EIN\", \"NAME\"], axis=1)\n",
    "\n",
    "# df = df[[\"AFFILIATION\", \"APPLICATION_TYPE\", \"CLASSIFICATION\", \"ORGANIZATION\", \"IS_SUCCESSFUL\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d896740e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title(\"Title\")\n",
    "ax1.set_ylabel(\"Y label\")\n",
    "ax1.boxplot(df[\"ASK_AMT\"].unique())\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cats(a_col, a_cutoff):\n",
    "    \"\"\" Inputs are a series and a cutoff value for 'Other' \"\"\"\n",
    "    \n",
    "    print(f\"{a_col} BEFORE: \\n\\n{df[a_col].value_counts()}\\n\\n\")\n",
    "\n",
    "    types_to_replace = (df[a_col].value_counts().loc[lambda x: x < int(a_cutoff)]).keys().tolist()\n",
    "\n",
    "    for code in types_to_replace:        \n",
    "        df[a_col] = df[a_col].replace(code, \"Other\")\n",
    "\n",
    "    # Check to make sure binning was successful\n",
    "\n",
    "    print(f\"{a_col} AFTER: \\n\\n{df[a_col].value_counts()}\\n\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_cats(\"APPLICATION_TYPE\", 500)\n",
    "\n",
    "reduce_cats(\"ASK_AMT\", 25_000)\n",
    "\n",
    "reduce_cats(\"CLASSIFICATION\", 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "\n",
    "alphasoup_cat = df.dtypes[df.dtypes == \"object\"].index.tolist() \n",
    "alphasoup_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_df(a_df):\n",
    "    \"\"\" Returns one-hot encoded dataframe \"\"\"\n",
    "    \n",
    "    categorical_list = a_df.dtypes[a_df.dtypes == \"object\"].index.tolist() \n",
    "    \n",
    "    print(f\"CATEGORIES FOR EACH CATEGORICAL FEATURE ENCODED:\\n\\n{a_df[categorical_list].nunique()}\\n\\n\")\n",
    "    \n",
    "    concat_list = []\n",
    "    \n",
    "    for categorical in categorical_list:\n",
    "        \n",
    "        concat_list.append(pd.get_dummies(a_df[categorical], prefix=categorical, prefix_sep='_'))        \n",
    "    \n",
    "    concat_list.append(a_df[\"IS_SUCCESSFUL\"])\n",
    "\n",
    "    return pd.concat(concat_list, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = encode_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "\n",
    "y = df[\"IS_SUCCESSFUL\"]\n",
    "X = df.drop([\"IS_SUCCESSFUL\"], axis=1)\n",
    "\n",
    "best_features = SelectKBest(score_func=chi2, k=5)\n",
    "fit = best_features.fit(X, y)\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "feature_Scores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "feature_Scores.columns = [\"Specs\", \"Score\"]\n",
    "\n",
    "print(feature_Scores.nlargest(5, \"Score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b26fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"\\n\\n\", model.feature_importances_)\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "feat_importances.nlargest(5).plot(kind=\"barh\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09532a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df.corr()\n",
    "\n",
    "top_corr_features = corrmat.index\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "sns.heatmap(df[top_corr_features].corr(), annot=False, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718782b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=1, n_estimators=128).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'Training Score: {clf.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f98d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = clf.feature_importances_ \n",
    "\n",
    "features = sorted(zip(df.columns, clf.feature_importances_), key = lambda x: x[1])\n",
    "cols = [f[0] for f in features]\n",
    "width = [f[1] for f in features]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10,200)\n",
    "plt.margins(y=0.001)\n",
    "\n",
    "ax.barh(y=cols, width=width)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3332503",
   "metadata": {},
   "source": [
    "### AFFIL, ORG, APP_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30888cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = SelectFromModel(clf)\n",
    "sel.fit(X_train_scaled, y_train)\n",
    "\n",
    "X_selected_train, X_selected_test, y_train, y_test = train_test_split(sel.transform(X), y, random_state=1)\n",
    "scaler = StandardScaler().fit(X_selected_train)\n",
    "X_selected_train_scaled = scaler.transform(X_selected_train)\n",
    "X_selected_test_scaled = scaler.transform(X_selected_test)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_selected_train_scaled, y_train)\n",
    "print(f'Training Score: {clf.score(X_selected_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf.score(X_selected_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b361fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=63, input_dim=21, activation=\"tanh\"))\n",
    "\n",
    "# Second hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=63, activation=\"tanh\"))\n",
    "\n",
    "# Output layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"tanh\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1187bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "\n",
    "# nn.save(\"nn_optimized.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
