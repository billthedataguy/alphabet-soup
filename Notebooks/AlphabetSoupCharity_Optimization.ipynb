{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01270e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d35c3",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\" Reads dataset csv and returns pandas dataframe \"\"\"\n",
    "    \n",
    "    filepath = \"../Resources/charity_data.csv\"\n",
    "\n",
    "    df = pd.read_csv(filepath, encoding=\"utf-8\", low_memory=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(a_df):\n",
    "    \"\"\" Returns deduped, na-dropped, useless column dropped, index-reset dataframe \"\"\"    \n",
    "    \n",
    "    a_df = a_df.drop_duplicates()   \n",
    "        \n",
    "    a_df = a_df.dropna()\n",
    "    \n",
    "    a_df = a_df.drop(columns=[\"EIN\", \"NAME\"])\n",
    "        \n",
    "    a_df = a_df.reset_index(drop=True)\n",
    "    \n",
    "    return a_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_dataset(a_df):\n",
    "    \"\"\" Provides summary info and value counts of columns with > 10 unique values \"\"\"\n",
    "    \n",
    "    a_df.info()\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "                  \n",
    "    # Determine the number of unique values in each column.\n",
    "\n",
    "    for col in a_df.columns:        \n",
    "        if (a_df[col].nunique() > 10):\n",
    "            print(f\"{col}\\n\\n{a_df[col].value_counts()}\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b7acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cats(a_col, a_cutoff):\n",
    "    \"\"\" Inputs are a series and a cutoff value for 'Other' \"\"\"\n",
    "    \n",
    "    print(f\"BEFORE: \\n\\n{df[a_col].value_counts()}\\n\\n\")\n",
    "\n",
    "    types_to_replace = (df[a_col].value_counts().loc[lambda x: x < int(a_cutoff)]).keys().tolist()\n",
    "\n",
    "    for code in types_to_replace:        \n",
    "        df[a_col] = df[a_col].replace(code, \"Other\")\n",
    "\n",
    "    # Check to make sure binning was successful\n",
    "\n",
    "    print(f\"AFTER: \\n\\n{df[a_col].value_counts()}\\n\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_df(a_df):\n",
    "    \"\"\" Returns one-hot encoded dataframe \"\"\"\n",
    "    \n",
    "    categorical_list = a_df.dtypes[a_df.dtypes == \"object\"].index.tolist() \n",
    "    \n",
    "    print(f\"CATEGORIES FOR EACH CATEGORICAL FEATURE ENCODED:\\n\\n{a_df[categorical_list].nunique()}\\n\\n\")\n",
    "    \n",
    "    concat_list = []\n",
    "    \n",
    "    for categorical in categorical_list:\n",
    "        \n",
    "        concat_list.append(pd.get_dummies(a_df[categorical], prefix=categorical, prefix_sep='_'))        \n",
    "    \n",
    "    concat_list.append(a_df[\"IS_SUCCESSFUL\"])\n",
    "\n",
    "    return pd.concat(concat_list, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(a_df):\n",
    "    \"\"\" Make X,y ... train_test_split ... scale, fit and transform \"\"\"\n",
    "    \n",
    "    # Split our preprocessed data into our features and target arrays\n",
    "\n",
    "    y = a_df[\"IS_SUCCESSFUL\"]\n",
    "    X = a_df.drop([\"IS_SUCCESSFUL\"], axis=1)\n",
    "\n",
    "    # Split the preprocessed data into a training and testing dataset\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) \n",
    "    \n",
    "    # Create a StandardScaler instances\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the StandardScaler\n",
    "\n",
    "    X_scaler = scaler.fit(X_train)\n",
    "\n",
    "    # Scale the data\n",
    "\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    print(f\"TRAIN SCALED SHAPE: {X_train_scaled.shape}\")\n",
    "    print(f\"TEST SCALED SHAPE: {X_test_scaled.shape}\")\n",
    "\n",
    "    input_dim = X_train_scaled.shape[1]  \n",
    "    \n",
    "    return input_dim, X_train_scaled, X_test_scaled, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn(input_dim=43, num_layers=2, num_units=100, num_epochs=10):\n",
    "    \"\"\" Makes sequential nn, compiles, fits, saves, and reports on loss and accuracy \"\"\"    \n",
    "    \n",
    "    nn = tf.keras.models.Sequential()\n",
    "\n",
    "    # First layer \n",
    "    \n",
    "    nn.add(tf.keras.layers.Dense(units=num_units, input_dim=input_dim, activation=\"relu\"))\n",
    "    \n",
    "    # Hidden layers\n",
    "    \n",
    "    for layer in range(1, num_layers):\n",
    "        \n",
    "        nn.add(tf.keras.layers.Dense(units=num_units, activation=\"relu\"))\n",
    "\n",
    "    # Output layer\n",
    "\n",
    "    nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    \n",
    "    nn.summary()\n",
    "    \n",
    "    nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    fit_model = nn.fit(X_train_scaled, y_train, epochs=num_epochs)   \n",
    "    \n",
    "    model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "    \n",
    "    print(f\"\\n\\nLoss: {model_loss}, Accuracy: {model_accuracy}\")    \n",
    "\n",
    "    nn.save(\"../Models/AlphabetSoupCharity_Optimization.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af6971f",
   "metadata": {},
   "source": [
    "# Tinkerings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d1bceb",
   "metadata": {},
   "source": [
    "## (1) Examine correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe and examine\n",
    "\n",
    "enc_df = encode_df(clean_dataset(load_dataset()))\n",
    "\n",
    "# Examine correlations from encoded dataframe\n",
    "\n",
    "enc_df.corr()\n",
    "\n",
    "corrmat = enc_df.corr()\n",
    "\n",
    "top_corr_features = corrmat.index\n",
    "\n",
    "plt.figure(figsize=(20,50))\n",
    "\n",
    "sns.heatmap(enc_df[top_corr_features].corr()[[\"IS_SUCCESSFUL\"]].sort_values(by=\"IS_SUCCESSFUL\", ascending=False),\\\n",
    "            vmin=-1, vmax=1, annot=True, cmap=\"crest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5eda77",
   "metadata": {},
   "source": [
    "## (2) Random Forest Classifier and feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataframe\n",
    "\n",
    "input_dim, X_train_scaled, X_test_scaled, y_train, y_test = pre_process(enc_df)\n",
    "\n",
    "# See what Random Forest offers\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=200).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"RandomForestClassifier Training Score: {clf.score(X_train_scaled, y_train)}\")\n",
    "print(f\"RandomForestClassifier Testing Score: {clf.score(X_test_scaled, y_test)}\")\n",
    "\n",
    "feature_importances = clf.feature_importances_ \n",
    "\n",
    "features = sorted(zip(enc_df.columns, clf.feature_importances_), key = lambda x: x[1])\n",
    "cols = [f[0] for f in features]\n",
    "width = [f[1] for f in features]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10,50)\n",
    "plt.margins(y=0.01)\n",
    "\n",
    "ax.barh(y=cols, width=width)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Now try with the selected features\n",
    "\n",
    "sel = SelectFromModel(clf).fit(X_train_scaled, y_train)\n",
    "\n",
    "X_selected_train, X_selected_test, y_train, y_test = train_test_split(sel.transform(X_train_scaled), y_train, random_state=0)\n",
    "\n",
    "scaler = StandardScaler().fit(X_selected_train)\n",
    "X_selected_train_scaled = scaler.transform(X_selected_train)\n",
    "X_selected_test_scaled = scaler.transform(X_selected_test)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=200).fit(X_selected_train_scaled, y_train)\n",
    "\n",
    "print(f\"SelectFromModel RandomForestClassifier Training Score: {clf.score(X_selected_train_scaled, y_train)}\")\n",
    "print(f\"SelectFromModel RandomForestClassifier Testing Score: {clf.score(X_selected_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb2bc2a",
   "metadata": {},
   "source": [
    "## (3) Extra Trees Classifier and feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4baa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe and examine\n",
    "\n",
    "enc_df = encode_df(clean_dataset(load_dataset()))\n",
    "\n",
    "# Preprocess dataframe\n",
    "\n",
    "input_dim, X_train_scaled, X_test_scaled, y_train, y_test = pre_process(enc_df)\n",
    "\n",
    "# See what Extra Trees offers\n",
    "\n",
    "model = ExtraTreesClassifier().fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\n\\nExtraTreesClassifier Training Score: {model.score(X_train_scaled, y_train)}\")\n",
    "print(f\"ExtraTreesClassifier Testing Score: {model.score(X_test_scaled, y_test)}\")\n",
    "\n",
    "feature_importances = model.feature_importances_ \n",
    "\n",
    "features = sorted(zip(enc_df.columns, model.feature_importances_), key = lambda x: x[1])\n",
    "cols = [f[0] for f in features]\n",
    "width = [f[1] for f in features]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10,50)\n",
    "plt.margins(y=0.01)\n",
    "\n",
    "ax.barh(y=cols, width=width)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Now try with the selected features\n",
    "\n",
    "sel = SelectFromModel(model).fit(X_train_scaled, y_train)\n",
    "\n",
    "X_selected_train, X_selected_test, y_train, y_test = train_test_split(sel.transform(X_train_scaled), y_train, random_state=0)\n",
    "\n",
    "scaler = StandardScaler().fit(X_selected_train)\n",
    "X_selected_train_scaled = scaler.transform(X_selected_train)\n",
    "X_selected_test_scaled = scaler.transform(X_selected_test)\n",
    "\n",
    "model = ExtraTreesClassifier().fit(X_selected_train_scaled, y_train)\n",
    "\n",
    "print(f\"SelectFromModel ExtraTreesClassifier Training Score: {model.score(X_selected_train_scaled, y_train)}\")\n",
    "print(f\"SelectFromModel ExtraTreesClassifier Testing Score: {model.score(X_selected_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ead49",
   "metadata": {},
   "source": [
    "# Return to neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdacab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe and examine\n",
    "\n",
    "df = clean_dataset(load_dataset())\n",
    "\n",
    "examine_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a11b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin columns with > 10 unique values\n",
    "\n",
    "reduce_cats(\"APPLICATION_TYPE\", 1000)\n",
    "\n",
    "reduce_cats(\"CLASSIFICATION\", 17326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24d50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"AFFILIATION\", \"APPLICATION_TYPE\", \"CLASSIFICATION\", \"INCOME_AMT\", \"ORGANIZATION\", \"USE_CASE\", \"IS_SUCCESSFUL\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b7f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df = encode_df(df)\n",
    "enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataframe\n",
    "\n",
    "input_dim, X_train_scaled, X_test_scaled, y_train, y_test = pre_process(enc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb48c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_nn(32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
