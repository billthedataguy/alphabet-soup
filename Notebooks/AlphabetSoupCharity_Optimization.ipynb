{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01270e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_selection import chi2, f_classif, SelectFromModel, SelectKBest \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191d35c3",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\" Reads dataset csv and returns pandas dataframe \"\"\"\n",
    "    \n",
    "    filepath = \"../Resources/charity_data.csv\"\n",
    "\n",
    "    df = pd.read_csv(filepath, encoding=\"utf-8\", low_memory=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(a_df):\n",
    "    \"\"\" Returns deduped, na-dropped, useless column dropped, index-reset dataframe \"\"\"    \n",
    "    \n",
    "    a_df = a_df.drop_duplicates()   \n",
    "        \n",
    "    a_df = a_df.dropna()\n",
    "    \n",
    "    a_df = a_df.drop(columns=[\"EIN\", \"NAME\"])\n",
    "        \n",
    "    a_df = a_df.reset_index(drop=True)\n",
    "    \n",
    "    return a_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_dataset(a_df):\n",
    "    \"\"\" Provides summary info and visualizations of dataset \"\"\"\n",
    "    \n",
    "    a_df.info()\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "                  \n",
    "    # Determine the number of unique values in each column.\n",
    "\n",
    "    for col in a_df.columns:        \n",
    "        if (a_df[col].nunique() > 10):\n",
    "            print(f\"{col}\\n\\n{a_df[col].value_counts()}\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cats(a_col, a_cutoff):\n",
    "    \"\"\" Inputs are a series and a cutoff value for 'Other' \"\"\"\n",
    "    \n",
    "    print(f\"BEFORE: \\n\\n{df[a_col].value_counts()}\\n\\n\")\n",
    "\n",
    "    types_to_replace = (df[a_col].value_counts().loc[lambda x: x < int(a_cutoff)]).keys().tolist()\n",
    "\n",
    "    for code in types_to_replace:        \n",
    "        df[a_col] = df[a_col].replace(code, \"Other\")\n",
    "\n",
    "    # Check to make sure binning was successful\n",
    "\n",
    "    print(f\"AFTER: \\n\\n{df[a_col].value_counts()}\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_df(a_df):\n",
    "    \"\"\" Returns one-hot encoded dataframe \"\"\"\n",
    "    \n",
    "    categorical_list = a_df.dtypes[a_df.dtypes == \"object\"].index.tolist() \n",
    "    \n",
    "    print(f\"CATEGORIES FOR EACH CATEGORICAL FEATURE ENCODED:\\n\\n{a_df[categorical_list].nunique()}\\n\\n\")\n",
    "    \n",
    "    concat_list = []\n",
    "    \n",
    "    for categorical in categorical_list:\n",
    "        \n",
    "        concat_list.append(pd.get_dummies(a_df[categorical], prefix=categorical, prefix_sep='_'))        \n",
    "    \n",
    "    concat_list.append(a_df[\"IS_SUCCESSFUL\"])\n",
    "\n",
    "    return pd.concat(concat_list, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(a_df):\n",
    "    \"\"\" Make X,y ... train_test_split ... scale, fit and transform \"\"\"\n",
    "    \n",
    "    # Split our preprocessed data into our features and target arrays\n",
    "\n",
    "    y = enc_df[\"IS_SUCCESSFUL\"]\n",
    "    X = enc_df.drop([\"IS_SUCCESSFUL\"], axis=1)\n",
    "\n",
    "    # Split the preprocessed data into a training and testing dataset\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "    \n",
    "    # Create a StandardScaler instances\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the StandardScaler\n",
    "\n",
    "    X_scaler = scaler.fit(X_train)\n",
    "\n",
    "    # Scale the data\n",
    "\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    print(f\"TRAIN SCALED SHAPE: {X_train_scaled.shape}\")\n",
    "    print(f\"TEST SCALED SHAPE: {X_test_scaled.shape}\")\n",
    "\n",
    "    input_dim = X_train_scaled.shape[1]  \n",
    "    \n",
    "    return input_dim, X_train_scaled, X_test_scaled, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_nn(input_dim=43, num_layers=2, num_units=100, num_epochs=10):\n",
    "    \"\"\" Makes sequential nn, compiles, fits, saves, and reports on loss and accuracy \"\"\"    \n",
    "    \n",
    "    nn = tf.keras.models.Sequential()\n",
    "\n",
    "    # First layer \n",
    "    \n",
    "    nn.add(tf.keras.layers.Dense(units=num_units, input_dim=input_dim, activation=\"relu\"))\n",
    "    \n",
    "    # Hidden layers\n",
    "    \n",
    "    for layer in range(1, num_layers):\n",
    "        \n",
    "        nn.add(tf.keras.layers.Dense(units=num_units, activation=\"relu\"))\n",
    "\n",
    "    # Output layer\n",
    "\n",
    "    nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    \n",
    "    nn.summary()\n",
    "    \n",
    "    nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) \n",
    "    \n",
    "    fit_model = nn.fit(X_train_scaled, y_train, epochs=num_epochs)   \n",
    "    \n",
    "    model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "    \n",
    "    print(f\"\\n\\nLoss: {model_loss}, Accuracy: {model_accuracy}\")    \n",
    "\n",
    "    nn.save(\"../Models/AlphabetSoupCharity_Optimization.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf0194b",
   "metadata": {},
   "source": [
    "# Call functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframe and examine\n",
    "\n",
    "df = clean_dataset(load_dataset())\n",
    "\n",
    "examine_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin columns with > 10 unique values\n",
    "\n",
    "reduce_cats(\"APPLICATION_TYPE\", 500)\n",
    "\n",
    "reduce_cats(\"ASK_AMT\", 25_000)\n",
    "\n",
    "reduce_cats(\"CLASSIFICATION\", 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4df26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df = encode_df(df)\n",
    "\n",
    "enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb0e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataframe\n",
    "\n",
    "input_dim, X_train_scaled, X_test_scaled, y_train, y_test = pre_process(enc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_nn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0340963",
   "metadata": {},
   "source": [
    "# Tinkerings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = SelectKBest(score_func=chi2, k=5)\n",
    "fit = best_features.fit(X, y)\n",
    "\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "feature_Scores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "feature_Scores.columns = [\"Specs\", \"Score\"]\n",
    "\n",
    "print(feature_Scores.nlargest(5, \"Score\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b26fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"\\n\\n\", model.feature_importances_)\n",
    "\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\n",
    "feat_importances.nlargest(5).plot(kind=\"barh\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09532a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df.corr()\n",
    "\n",
    "top_corr_features = corrmat.index\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "sns.heatmap(df[top_corr_features].corr(), annot=False, cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718782b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0, n_estimators=128).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'Training Score: {clf.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f98d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = clf.feature_importances_ \n",
    "\n",
    "features = sorted(zip(df.columns, clf.feature_importances_), key = lambda x: x[1])\n",
    "cols = [f[0] for f in features]\n",
    "width = [f[1] for f in features]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10,200)\n",
    "plt.margins(y=0.001)\n",
    "\n",
    "ax.barh(y=cols, width=width)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30888cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = SelectFromModel(clf)\n",
    "sel.fit(X_train_scaled, y_train)\n",
    "\n",
    "X_selected_train, X_selected_test, y_train, y_test = train_test_split(sel.transform(X), y, random_state=1)\n",
    "scaler = StandardScaler().fit(X_selected_train)\n",
    "X_selected_train_scaled = scaler.transform(X_selected_train)\n",
    "X_selected_test_scaled = scaler.transform(X_selected_test)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_selected_train_scaled, y_train)\n",
    "print(f'Training Score: {clf.score(X_selected_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf.score(X_selected_test_scaled, y_test)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
