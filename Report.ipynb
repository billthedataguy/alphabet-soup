{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383d8362",
   "metadata": {},
   "source": [
    "# Report on the Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d678e75",
   "metadata": {},
   "source": [
    "**Overview of the analysis**: Explain the purpose of this analysis.\n",
    "\n",
    "**Results**: Using bulleted lists and images to support your answers, address the following questions.\n",
    "\n",
    "**Data Preprocessing**\n",
    "\n",
    "What variable(s) are the target(s) for your model?\n",
    "What variable(s) are the features for your model?\n",
    "What variable(s) should be removed from the input data because they are neither targets nor features?\n",
    "\n",
    "**Compiling, Training, and Evaluating the Model**\n",
    "\n",
    "How many neurons, layers, and activation functions did you select for your neural network model, and why?\n",
    "Were you able to achieve the target model performance?\n",
    "What steps did you take in your attempts to increase model performance?\n",
    "\n",
    "**Summary**: Summarize the overall results of the deep learning model. Include a recommendation for how a different model could solve this classification problem, and then explain your recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01270e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d0e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import and read the charity_data.csv\n",
    "\n",
    "df = pd.read_csv(\"Resources/charity_data.csv\")\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "\n",
    "df = df[[\"AFFILIATION\", \"ORGANIZATION\", \"APPLICATION_TYPE\", \"IS_SUCCESSFUL\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b7a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_cats(a_col, a_cutoff):\n",
    "    \"\"\" Inputs are a series and a cutoff value for 'Other' \"\"\"\n",
    "    \n",
    "    print(f\"BEFORE: \\n\\n{df[a_col].value_counts()}\\n\\n\")\n",
    "\n",
    "    types_to_replace = (df[a_col].value_counts().loc[lambda x: x < int(a_cutoff)]).keys().tolist()\n",
    "\n",
    "    for code in types_to_replace:        \n",
    "        df[a_col] = df[a_col].replace(code, \"Other\")\n",
    "\n",
    "    # Check to make sure binning was successful\n",
    "\n",
    "    print(f\"AFTER: \\n\\n{df[a_col].value_counts()}\\n\\n\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_cats(\"APPLICATION_TYPE\", 10_000)\n",
    "\n",
    "reduce_cats(\"AFFILIATION\", 10_000)\n",
    "\n",
    "reduce_cats(\"CLASSIFICATION\", 10_000)\n",
    "\n",
    "reduce_cats(\"USE_CASE\", 10_000)\n",
    "\n",
    "reduce_cats(\"ORGANIZATION\", 10_000)\n",
    "\n",
    "reduce_cats(\"ASK_AMT\", 10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric with `pd.get_dummies`\n",
    "\n",
    "alphasoup_cat = df.dtypes[df.dtypes == \"object\"].index.tolist() \n",
    "alphasoup_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.get_dummies(df[\"APPLICATION_TYPE\"], prefix=\"APP_TYPE\", prefix_sep='_')\n",
    "df2 = pd.get_dummies(df[\"AFFILIATION\"], prefix=\"AFFIL\", prefix_sep='_')\n",
    "df3 = pd.get_dummies(df[\"ORGANIZATION\"], prefix=\"ORG\", prefix_sep='_')\n",
    "\n",
    "\n",
    "# df3 = pd.get_dummies(df[\"CLASSIFICATION\"], prefix=\"CLASS\", prefix_sep='_')\n",
    "# df4 = pd.get_dummies(df[\"USE_CASE\"], prefix=\"USE_CASE\", prefix_sep='_')\n",
    "# df6 = pd.get_dummies(df[\"INCOME_AMT\"], prefix=\"INCOME\", prefix_sep='_')\n",
    "# df7 = pd.get_dummies(df[\"SPECIAL_CONSIDERATIONS\"], prefix=\"SPECIAL_CONSID\", prefix_sep='_')\n",
    "# df8 = pd.get_dummies(df[\"ASK_AMT\"], prefix=\"ASK_AMT\", prefix_sep='_')\n",
    "# df9 = pd.get_dummies(df[\"STATUS\"], prefix=\"STATUS\", prefix_sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_df = pd.concat([df1, df2, df3, df[\"IS_SUCCESSFUL\"]], axis=1)\n",
    "print(enc_df.info())\n",
    "enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "\n",
    "y = enc_df[\"IS_SUCCESSFUL\"].values\n",
    "X = enc_df.drop([\"IS_SUCCESSFUL\"], axis=1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "print(X_train_scaled.shape)\n",
    "print(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718782b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'Training Score: {clf.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f98d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = clf.feature_importances_ \n",
    "\n",
    "features = sorted(zip(enc_df.columns, clf.feature_importances_), key = lambda x: x[1])\n",
    "cols = [f[0] for f in features]\n",
    "width = [f[1] for f in features]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_size_inches(10,200)\n",
    "plt.margins(y=0.001)\n",
    "\n",
    "ax.barh(y=cols, width=width)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3332503",
   "metadata": {},
   "source": [
    "### AFFIL, ORG, APP_TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30888cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = SelectFromModel(clf)\n",
    "sel.fit(X_train_scaled, y_train)\n",
    "\n",
    "X_selected_train, X_selected_test, y_train, y_test = train_test_split(sel.transform(X), y, random_state=1)\n",
    "scaler = StandardScaler().fit(X_selected_train)\n",
    "X_selected_train_scaled = scaler.transform(X_selected_train)\n",
    "X_selected_test_scaled = scaler.transform(X_selected_test)\n",
    "\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_selected_train_scaled, y_train)\n",
    "print(f'Training Score: {clf.score(X_selected_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf.score(X_selected_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c2102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48207b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412105d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b361fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=60, input_dim=27, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=60, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a33907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1187bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "\n",
    "nn.save(\"nn_optimized.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c40b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
